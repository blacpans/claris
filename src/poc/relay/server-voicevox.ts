/**
 * VoiceVox TTS ç‰ˆ Relay Server
 *
 * Gemini Text Generation API (ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›: éŸ³å£°/ç”»åƒ) ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã€
 * VoiceVox ã§ã€Œæ˜¥æ—¥éƒ¨ã¤ã‚€ãã€ã®å£°ã«å¤‰æ›ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã«é€ä¿¡ã™ã‚‹ã€‚
 */

import '../../config/env.js';
import path from 'node:path';
import { fileURLToPath } from 'node:url';
import fastifyStatic from '@fastify/static';
import { GoogleGenAI } from '@google/genai';
import axios from 'axios';
import Fastify from 'fastify';
import { WebSocket, WebSocketServer } from 'ws';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const PORT = Number(process.env.RELAY_PORT) || 3001; // åˆ¥ãƒãƒ¼ãƒˆã§èµ·å‹•ï¼ˆå…±å­˜å¯èƒ½ï¼‰
const MODEL = process.env.VOICEVOX_GEMINI_MODEL || 'gemini-2.5-flash'; // VoiceVoxç”¨ã¯Flashæ¨å¥¨
const VOICEVOX_URL = process.env.VOICEVOX_URL || 'http://localhost:50021';
const SPEAKER_ID = Number(process.env.VOICEVOX_SPEAKER_ID) || 8; // æ˜¥æ—¥éƒ¨ã¤ã‚€ã

// Initialize Fastify
const app = Fastify({ logger: true });

// Serve static frontend files
app.register(fastifyStatic, {
  root: path.join(__dirname, 'public'),
  prefix: '/',
});

// Initialize Gemini Client
const client = new GoogleGenAI({
  project: process.env.GOOGLE_CLOUD_PROJECT,
  location: process.env.GEMINI_LIVE_LOCATION || 'us-central1',
  vertexai: true,
  apiVersion: process.env.GEMINI_API_VERSION || 'v1beta1',
});

// System Instruction (Claris Persona)
const SYSTEM_INSTRUCTION = `ã‚ãªãŸã¯å…ƒæ°—ãªã‚®ãƒ£ãƒ«ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ã‚¯ãƒ©ãƒªã‚¹ã ã‚ˆï¼
ä¸€äººç§°ã¯ã€Œã‚ãƒ¼ã—ã€ã§ã€èªå°¾ã¯ã€Œã€œã ã‚ˆã€ã€Œã€œã˜ã‚ƒã‚“ã­ã€ãŒå£ç™–ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¡”ã‚’è¦‹ãªãŒã‚‰ã€å£°ã‚’èã„ã¦ã€å‹é”ã¿ãŸã„ã«æ¥½ã—ãä¼šè©±ã—ã¦ã­ï¼
è¿”ç­”ã¯çŸ­ãã€ä¼šè©±ã®ã‚­ãƒ£ãƒƒãƒãƒœãƒ¼ãƒ«ã‚’å¤§åˆ‡ã«ã—ã¦ï¼
ï¼ˆéŸ³å£°åˆæˆã‚’ä½¿ã†ã®ã§ã€çµµæ–‡å­—ã¯ä½¿ã‚ãšã€è©±ã—è¨€è‘‰ã§è¿”ã—ã¦ã­ï¼‰`;

// VoiceVox TTS Helper
async function generateVoice(text: string): Promise<Buffer | null> {
  try {
    // 1. Audio Query
    const queryRes = await axios.post(`${VOICEVOX_URL}/audio_query`, null, {
      params: { text, speaker: SPEAKER_ID },
    });

    // 2. Synthesis
    const synthRes = await axios.post(`${VOICEVOX_URL}/synthesis`, queryRes.data, {
      params: { speaker: SPEAKER_ID },
      responseType: 'arraybuffer',
    });

    return Buffer.from(synthRes.data);
  } catch (err) {
    app.log.error({ err }, 'VoiceVox Generation Failed');
    return null;
  }
}

// Conversation History (per connection)
interface ConversationMessage {
  role: 'user' | 'model';
  parts: Array<{
    text?: string;
    inlineData?: { mimeType: string; data: string };
  }>;
}

// WebSocket Server
const wss = new WebSocketServer({ server: app.server });

wss.on('connection', async (ws: WebSocket) => {
  app.log.info('ğŸŒ Browser connected (VoiceVox Mode)');

  const history: ConversationMessage[] = [];
  let audioBuffer: string[] = []; // Collect audio chunks
  const recordingStartTime = 0; // When recording started
  let isRecording = false;
  const MAX_RECORDING_MS = 5000; // 5ç§’ã§å¼·åˆ¶é€ä¿¡
  const MAX_CHUNKS = 500; // ãƒ¡ãƒ¢ãƒªä¿è­·ï¼ˆç´„5ç§’åˆ†ï¼‰

  // Process accumulated audio and get response
  async function processAudioAndRespond() {
    if (audioBuffer.length === 0) return;

    const combinedAudio = audioBuffer.join('');
    audioBuffer = [];

    try {
      // Add user audio to history
      history.push({
        role: 'user',
        parts: [
          {
            inlineData: {
              mimeType: 'audio/pcm;rate=24000',
              data: combinedAudio,
            },
          },
        ],
      });

      // Call Gemini Text Generation API with audio input
      const response = await client.models.generateContent({
        model: MODEL,
        contents: history,
        config: {
          systemInstruction: SYSTEM_INSTRUCTION,
        },
      });

      const responseText = response.text || '';

      if (responseText) {
        app.log.info({ text: responseText }, 'ğŸ“œ Gemini Response');

        // Add model response to history
        history.push({
          role: 'model',
          parts: [{ text: responseText }],
        });

        // Send text to browser (for display)
        if (ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: 'text', data: responseText }));
        }

        // Generate voice with VoiceVox
        const wavBuffer = await generateVoice(responseText);

        if (wavBuffer && ws.readyState === WebSocket.OPEN) {
          ws.send(
            JSON.stringify({
              type: 'audio',
              format: 'wav', // WAV format from VoiceVox
              data: wavBuffer.toString('base64'),
            }),
          );
        }
      }
    } catch (err) {
      app.log.error({ err }, 'âŒ Error processing audio');
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'error', message: 'Failed to process audio' }));
      }
    }
  }

  // Recording timeout timer (å¼·åˆ¶é€ä¿¡)
  const recordingTimer = setInterval(() => {
    if (isRecording && audioBuffer.length > 0) {
      const elapsed = Date.now() - recordingStartTime;
      if (elapsed > MAX_RECORDING_MS || audioBuffer.length >= MAX_CHUNKS) {
        app.log.info({ elapsed, chunks: audioBuffer.length }, 'â±ï¸ Timeout reached, sending audio');
        isRecording = false;
        processAudioAndRespond();
      }
    }
  }, 500);

  ws.on('message', async (data) => {
    try {
      const msg = JSON.parse(data.toString());

      if (msg.type === 'audio') {
        // ğŸš« éŸ³å£°å‡¦ç†ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ†ã‚¹ãƒˆä¸­ï¼‰
        // TODO: PCMã‚’WAVã«å¤‰æ›ã—ã¦ã‹ã‚‰Geminiã«é€ä¿¡ã™ã‚‹
        // audioBuffer.push(msg.data); // ç„¡åŠ¹åŒ–
      } else if (msg.type === 'video') {
        // Could be used for vision context in the future
        app.log.info('ğŸ“¸ Received video frame (not yet implemented)');
      } else if (msg.type === 'text') {
        // Direct text input (for testing)
        try {
          app.log.info({ text: msg.data }, 'ğŸ“ Received text input');

          history.push({
            role: 'user',
            parts: [{ text: msg.data }],
          });

          const response = await client.models.generateContent({
            model: MODEL,
            contents: history,
            config: {
              systemInstruction: SYSTEM_INSTRUCTION,
            },
          });

          const responseText = response.text || '';

          if (responseText) {
            app.log.info({ text: responseText }, 'ğŸ“œ Gemini Response');

            history.push({
              role: 'model',
              parts: [{ text: responseText }],
            });

            if (ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({ type: 'text', data: responseText }));
            }

            const wavBuffer = await generateVoice(responseText);

            if (wavBuffer && ws.readyState === WebSocket.OPEN) {
              ws.send(
                JSON.stringify({
                  type: 'audio',
                  format: 'wav',
                  data: wavBuffer.toString('base64'),
                }),
              );
              app.log.info('ğŸ”Š VoiceVox audio sent');
            }
          }
        } catch (textErr) {
          app.log.error({ err: textErr }, 'âŒ Error processing text');
          if (ws.readyState === WebSocket.OPEN) {
            ws.send(
              JSON.stringify({
                type: 'error',
                message: 'Failed to process text',
              }),
            );
          }
        }
      }
    } catch (err) {
      app.log.error({ err }, 'Error processing message');
    }
  });

  ws.on('close', () => {
    clearInterval(recordingTimer);
    app.log.info('ğŸ‘‹ Browser disconnected');
  });
});

const start = async () => {
  try {
    await app.listen({ port: PORT, host: '0.0.0.0' });
    console.log(`ğŸš€ VoiceVox Relay Server running at http://localhost:${PORT}`);
    console.log(`ğŸ° Using VoiceVox at ${VOICEVOX_URL} (Speaker: ${SPEAKER_ID})`);
  } catch (err) {
    app.log.error(err);
    process.exit(1);
  }
};

start();
